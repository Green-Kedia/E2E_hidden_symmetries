{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3171c791",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:09:38.460614Z",
     "iopub.status.busy": "2025-04-01T12:09:38.460258Z",
     "iopub.status.idle": "2025-04-01T12:09:44.720833Z",
     "shell.execute_reply": "2025-04-01T12:09:44.719961Z"
    },
    "papermill": {
     "duration": 6.267867,
     "end_time": "2025-04-01T12:09:44.722431",
     "exception": false,
     "start_time": "2025-04-01T12:09:38.454564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-ccc850f39093>:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  transform_model.load_state_dict(torch.load('/kaggle/input/deep-16-10-digit-1024-hd/transform_model.pth',map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Seed set to {seed}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class LatentTransformMLP(nn.Module):\n",
    "    \"\"\"MLP that learns rotation transformation in latent space\"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim=1024):\n",
    "        super(LatentTransformMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"Map latent vector to its next rotation state\"\"\"\n",
    "        return self.net(z)\n",
    "\n",
    "transform_model = LatentTransformMLP(16).to(device)\n",
    "transform_model.load_state_dict(torch.load('/kaggle/input/deep-16-10-digit-1024-hd/transform_model.pth',map_location=device))\n",
    "transform_model.eval()\n",
    "\n",
    "\n",
    "# 1. Data Loading and Exploration\n",
    "def load_data(file_path, digits=['digit_1','digit_2','digit_3']):\n",
    "    \"\"\"Load rotated MNIST data from HDF5 file.\"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Get all keys\n",
    "        print(\"Available keys:\", list(f.keys()))\n",
    "        \n",
    "        # Load data for each digit\n",
    "        data = {}\n",
    "        for key in f.keys():\n",
    "            if key.startswith('digit_') and key in digits:\n",
    "                data[key] = np.array(f[key])\n",
    "        \n",
    "        return data\n",
    "\n",
    "def plot_sample_digits(data):\n",
    "    \"\"\"Plot sample digits with different rotations.\"\"\"\n",
    "    n_digits = len(data)\n",
    "    fig, axes = plt.subplots(n_digits, 4, figsize=(15, 3 * n_digits))\n",
    "    if n_digits == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    fig.suptitle(\"Sample Digits with Different Rotations\", fontsize=16)\n",
    "    \n",
    "    for i, digit_key in enumerate(sorted(data.keys())):\n",
    "        # Select a random sample\n",
    "        sample_idx = np.random.randint(0, data[digit_key].shape[0])\n",
    "        \n",
    "        # Plot 4 different rotations\n",
    "        for j in range(4):\n",
    "            rotation_idx = j * 3  # 0, 3, 6, 9 (0, 90, 180, 270 degrees)\n",
    "            img = data[digit_key][sample_idx, rotation_idx, 0]\n",
    "            \n",
    "            axes[i, j].imshow(img, cmap='gray')\n",
    "            axes[i, j].set_title(f\"{digit_key}, Rotation: {rotation_idx*30}Â°\")\n",
    "            axes[i, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "def preprocess_data(data, test_size=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Preprocess data, combining all rotations and splitting into train/test.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary containing digit data\n",
    "        test_size: Fraction of data to use for testing\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test: Processed data arrays and labels\n",
    "    \"\"\"\n",
    "    X_all_list = []\n",
    "    y_all_list = []\n",
    "    \n",
    "    for digit_idx, key in enumerate(sorted(data.keys())):\n",
    "        digit_data = data[key]\n",
    "        n_samples, n_rotations, _, h, w = digit_data.shape\n",
    "        \n",
    "        # Extract data for all rotations\n",
    "        for rot_idx in range(n_rotations):\n",
    "            X = digit_data[:, rot_idx, 0]  # Shape: (n_samples, h, w)\n",
    "            X_all_list.append(X)\n",
    "            \n",
    "            # Create labels: (digit, rotation)\n",
    "            y = np.ones((X.shape[0], 2), dtype=np.int32)\n",
    "            y[:, 0] = digit_idx + 1  # Digit number (1-indexed)\n",
    "            y[:, 1] = rot_idx  # Rotation index\n",
    "            y_all_list.append(y)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    X_all = np.concatenate(X_all_list, axis=0)\n",
    "    y_all = np.concatenate(y_all_list, axis=0)\n",
    "    \n",
    "    # Perform train-test split with stratification by digit\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_all, y_all, test_size=test_size, random_state=seed, \n",
    "        shuffle=True, stratify=y_all[:,0]\n",
    "    )    \n",
    "    \n",
    "    # Reshape for PyTorch CNN (batch, channels, height, width)\n",
    "    X_train = X_train.reshape(-1, 1, h, w)\n",
    "    X_test = X_test.reshape(-1, 1, h, w)\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train.astype('float32') / np.max(X_train)\n",
    "    X_test = X_test.astype('float32') / np.max(X_train)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_dataloaders(X_train, X_test, y_train, y_test, batch_size=128):\n",
    "    \"\"\"Create PyTorch DataLoaders from numpy arrays\"\"\"\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long)\n",
    "    )\n",
    "    \n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.long)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, drop_last=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, drop_last=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# 3. VAE Model Implementation\n",
    "class VAEBase(nn.Module):\n",
    "    \"\"\"Base class for all VAE architectures\"\"\"\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAEBase, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"Map x to the parameters of the latent distribution\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement encode()\")\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Map latent samples to the reconstructed input\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement decode()\")\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"Reparameterization trick for sampling from latent space\"\"\"\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Full forward pass through the VAE\"\"\"\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, log_var, z\n",
    "    \n",
    "    def loss_function(self, recon_x, x, mu, log_var, kld_weight=1.0):\n",
    "        \"\"\"VAE loss function: reconstruction + KL divergence losses\"\"\"\n",
    "        recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "        kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        return recon_loss + kld_weight * kld_loss, recon_loss, kld_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"Sample from the latent space and decode\"\"\"\n",
    "        z = torch.randn(n_samples, self.latent_dim).to(device)\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def reconstruct(self, x):\n",
    "        \"\"\"Reconstruct input by encoding and then decoding\"\"\"\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon\n",
    "\n",
    "class ConvVAE(VAEBase):\n",
    "    \"\"\"Convolutional VAE for MNIST\"\"\"\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super(ConvVAE, self).__init__(latent_dim)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder_conv(x)\n",
    "        h = self.flatten(h)\n",
    "        return self.fc_mu(h), self.fc_log_var(h)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = self.decoder_input(z)\n",
    "        h = h.view(-1, 64, 7, 7)\n",
    "        return self.decoder_conv(h)\n",
    "\n",
    "class DeepVAE(VAEBase):\n",
    "    \"\"\"Deeper VAE with more conv layers\"\"\"\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super(DeepVAE, self).__init__(latent_dim)\n",
    "        \n",
    "        # Encoder with more layers\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_hidden = nn.Linear(128 * 7 * 7, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(256, latent_dim)\n",
    "        \n",
    "        # Decoder with more layers\n",
    "        self.decoder_input = nn.Linear(latent_dim, 256)\n",
    "        self.decoder_hidden = nn.Linear(256, 128 * 7 * 7)\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder_conv(x)\n",
    "        h = self.flatten(h)\n",
    "        h = self.relu(self.fc_hidden(h))\n",
    "        return self.fc_mu(h), self.fc_log_var(h)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = self.relu(self.decoder_input(z))\n",
    "        h = self.relu(self.decoder_hidden(h))\n",
    "        h = h.view(-1, 128, 7, 7)\n",
    "        return self.decoder_conv(h)\n",
    "\n",
    "\n",
    "# Factory function to create VAE models\n",
    "def create_vae_model(model_type, latent_dim):\n",
    "    if model_type == \"conv\":\n",
    "        return ConvVAE(latent_dim).to(device)\n",
    "    elif model_type == \"deep\":\n",
    "        return DeepVAE(latent_dim).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "# 4. Model Training\n",
    "def train_vae(model, train_loader, val_loader, optimizer, epochs, kld_weight=1.0, patience=5):\n",
    "    \"\"\"Train the VAE model\"\"\"\n",
    "    model.train()\n",
    "    train_history = {\n",
    "        \"loss\": [],\n",
    "        \"recon_loss\": [],\n",
    "        \"kld_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_recon_loss\": [],\n",
    "        \"val_kld_loss\": []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        train_recon_loss = 0\n",
    "        train_kld_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            recon_batch, mu, log_var, _ = model(data)\n",
    "            loss, recon, kld = model.loss_function(recon_batch, data, mu, log_var, kld_weight)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_recon_loss += recon.item()\n",
    "            train_kld_loss += kld.item()\n",
    "            \n",
    "        # Validation\n",
    "        val_loss, val_recon_loss, val_kld_loss = validate_vae(model, val_loader, kld_weight)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "        \n",
    "        # Record metrics\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        avg_train_recon_loss = train_recon_loss / len(train_loader.dataset)\n",
    "        avg_train_kld_loss = train_kld_loss / len(train_loader.dataset)\n",
    "        \n",
    "        train_history[\"loss\"].append(avg_train_loss)\n",
    "        train_history[\"recon_loss\"].append(avg_train_recon_loss)\n",
    "        train_history[\"kld_loss\"].append(avg_train_kld_loss)\n",
    "        train_history[\"val_loss\"].append(val_loss)\n",
    "        train_history[\"val_recon_loss\"].append(val_recon_loss)\n",
    "        train_history[\"val_kld_loss\"].append(val_kld_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Make sure we use the best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return train_history\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_vae(model, dataloader, kld_weight=1.0):\n",
    "    \"\"\"Validate the VAE model\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_recon_loss = 0\n",
    "    val_kld_loss = 0\n",
    "    \n",
    "    for data, _ in dataloader:\n",
    "        data = data.to(device)\n",
    "        recon_batch, mu, log_var, _ = model(data)\n",
    "        loss, recon, kld = model.loss_function(recon_batch, data, mu, log_var, kld_weight)\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        val_recon_loss += recon.item()\n",
    "        val_kld_loss += kld.item()\n",
    "    \n",
    "    return (\n",
    "        val_loss / len(dataloader.dataset),\n",
    "        val_recon_loss / len(dataloader.dataset),\n",
    "        val_kld_loss / len(dataloader.dataset)\n",
    "    )\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation loss.\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot total loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Total Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot reconstruction loss\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history[\"recon_loss\"], label=\"Training\")\n",
    "    plt.plot(history[\"val_recon_loss\"], label=\"Validation\")\n",
    "    plt.title(\"Reconstruction Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot KL loss\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history[\"kld_loss\"], label=\"Training\")\n",
    "    plt.plot(history[\"val_kld_loss\"], label=\"Validation\")\n",
    "    plt.title(\"KL Divergence\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 5. Latent Space Visualization\n",
    "@torch.no_grad()\n",
    "def encode_data(model, dataloader):\n",
    "    \"\"\"Encode data into latent space\"\"\"\n",
    "    model.eval()\n",
    "    z_means = []\n",
    "    z_log_vars = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for data, labels in dataloader:\n",
    "        data = data.to(device)\n",
    "        mu, log_var = model.encode(data)\n",
    "        z_means.append(mu.cpu().numpy())\n",
    "        z_log_vars.append(log_var.cpu().numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "    \n",
    "    z_means = np.vstack(z_means)\n",
    "    z_log_vars = np.vstack(z_log_vars)\n",
    "    labels = np.vstack(labels_list)\n",
    "    \n",
    "    return z_means, z_log_vars, labels\n",
    "\n",
    "def encode_and_plot_latent_space(model, dataloader, title=\"Latent Space Visualization\"):\n",
    "    \"\"\"\n",
    "    Encode data into latent space and visualize it.\n",
    "    \"\"\"\n",
    "    # Encode the data\n",
    "    z_mean, z_log_var, labels = encode_data(model, dataloader)\n",
    "    \n",
    "    # Project data to 2D if needed\n",
    "    if z_mean.shape[1] > 2:\n",
    "        z_mean = PCA(n_components=2).fit_transform(z_mean)\n",
    "    \n",
    "    # Extract digit and rotation labels\n",
    "    digit_labels = labels[:, 0]  # Digit (1, 2, or 3)\n",
    "    rotation_labels = labels[:, 1]  # Rotation index\n",
    "    \n",
    "    # Create a DataFrame for easier plotting\n",
    "    df = pd.DataFrame({\n",
    "        'z1': z_mean[:, 0],\n",
    "        'z2': z_mean[:, 1],\n",
    "        'digit': digit_labels,\n",
    "        'rotation': rotation_labels\n",
    "    })\n",
    "    \n",
    "    # Map rotation indices to degrees\n",
    "    rotation_degrees = {idx: idx * 30 for idx in range(12)}\n",
    "    df['rotation_degrees'] = df['rotation'].map(rotation_degrees)\n",
    "    \n",
    "    # Plot the latent space\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create a custom colormap for different digits\n",
    "    colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "    \n",
    "    # Plot each digit with different markers for different rotations\n",
    "    for digit in sorted(df['digit'].unique()):\n",
    "        digit_idx = int(digit) - 1  # Convert to 0-based index\n",
    "        if digit_idx < len(colors):\n",
    "            color = colors[digit_idx]\n",
    "        else:\n",
    "            color = None  # Let matplotlib choose\n",
    "            \n",
    "        digit_data = df[df['digit'] == digit]\n",
    "        \n",
    "        # Scatter plot\n",
    "        scatter = plt.scatter(\n",
    "            digit_data['z1'], \n",
    "            digit_data['z2'], \n",
    "            c=digit_data['rotation_degrees'], \n",
    "            marker=f\"${int(digit)}$\",  # Use digit as marker\n",
    "            alpha=0.7,\n",
    "            s=100,  # Marker size\n",
    "            cmap='viridis',\n",
    "            label=f\"Digit {int(digit)}\"\n",
    "        )\n",
    "    \n",
    "    # Add a colorbar for rotation\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('Rotation (degrees)')\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Latent Dimension 1\", fontsize=12)\n",
    "    plt.ylabel(\"Latent Dimension 2\", fontsize=12)\n",
    "    plt.legend(title=\"Digit\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return z_mean, labels\n",
    "\n",
    "def plot_rotation_trajectories(model, dataloader):\n",
    "    \"\"\"\n",
    "    Plot the trajectories of the same digit with different rotations in latent space.\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var, labels = encode_data(model, dataloader)\n",
    "    z_var = np.exp(z_log_var)\n",
    "    \n",
    "    # Project to 2D if needed\n",
    "    if z_mean.shape[1] > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        z_pca = pca.fit_transform(z_mean)\n",
    "        \n",
    "        # Project variance to PCA space\n",
    "        z_var_pca = np.dot(z_var, pca.components_.T)[:, :2]\n",
    "    else:\n",
    "        z_pca = z_mean\n",
    "        z_var_pca = z_var\n",
    "    \n",
    "    digit_labels = labels[:, 0]\n",
    "    rotation_labels = labels[:, 1]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'z1': z_pca[:, 0],\n",
    "        'z2': z_pca[:, 1],\n",
    "        'var1': z_var_pca[:, 0],\n",
    "        'var2': z_var_pca[:, 1],\n",
    "        'digit': digit_labels,\n",
    "        'rotation': rotation_labels\n",
    "    })\n",
    "    \n",
    "    rotation_degrees = {idx: idx * 30 for idx in range(12)}\n",
    "    df['rotation_degrees'] = df['rotation'].map(rotation_degrees)\n",
    "    \n",
    "    grouped = df.groupby(['digit', 'rotation_degrees']).agg({\n",
    "        'z1': 'mean',\n",
    "        'z2': 'mean',\n",
    "        'var1': 'mean',\n",
    "        'var2': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "    \n",
    "    for i, digit in enumerate(sorted(df['digit'].unique())):\n",
    "        digit_data = grouped[grouped['digit'] == digit]\n",
    "        if len(digit_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        if i < len(colors):\n",
    "            color = colors[i]\n",
    "        else:\n",
    "            color = None  # Let matplotlib choose\n",
    "            \n",
    "        digit_data = digit_data.sort_values('rotation_degrees')\n",
    "        \n",
    "        mean_z1, mean_z2 = digit_data['z1'], digit_data['z2']\n",
    "        var_z1, var_z2 = np.abs(digit_data['var1']), np.abs(digit_data['var2'])\n",
    "        \n",
    "        plt.plot(mean_z1, mean_z2, 'o-', color=color, \n",
    "                linewidth=2, markersize=8, label=f\"Digit {int(digit)}\")\n",
    "        \n",
    "        for j in range(len(mean_z1)):\n",
    "            ellipse = Ellipse(\n",
    "                (mean_z1.iloc[j], mean_z2.iloc[j]),\n",
    "                width=2 * np.sqrt(var_z1.iloc[j]),\n",
    "                height=2 * np.sqrt(var_z2.iloc[j]),\n",
    "                edgecolor=color,\n",
    "                facecolor='none',\n",
    "                alpha=0.5\n",
    "            )\n",
    "            plt.gca().add_patch(ellipse)\n",
    "            plt.text(mean_z1.iloc[j], mean_z2.iloc[j], \n",
    "                    f\"{int(digit_data.iloc[j]['rotation_degrees'])}Â°\", \n",
    "                    fontsize=9, ha='center', va='bottom', \n",
    "                    color=color)\n",
    "    \n",
    "    plt.title(\"Latent Space Trajectories with Model Variance\", fontsize=16)\n",
    "    plt.xlabel(\"Latent Dimension 1\", fontsize=12)\n",
    "    plt.ylabel(\"Latent Dimension 2\", fontsize=12)\n",
    "    plt.legend(title=\"Digit\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "# 6. Reconstruction Visualization\n",
    "@torch.no_grad()\n",
    "def plot_reconstructions(model, dataloader, n_samples=5):\n",
    "    \"\"\"\n",
    "    Plot original images and their reconstructions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained VAE model\n",
    "        dataloader: DataLoader for the data\n",
    "        n_samples: Number of samples to plot per digit\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of data\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for data, labels in dataloader:\n",
    "        data_list.append(data)\n",
    "        labels_list.append(labels)\n",
    "        if len(data_list) * dataloader.batch_size >= n_samples * 10:  # Collect enough samples\n",
    "            break\n",
    "    \n",
    "    data = torch.cat(data_list, dim=0)\n",
    "    labels = torch.cat(labels_list, dim=0)\n",
    "    \n",
    "    # Convert to numpy for easier handling\n",
    "    data_np = data.cpu().numpy()\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    \n",
    "    # Get reconstructions\n",
    "    data_device = data.to(device)\n",
    "    reconstructions = model.reconstruct(data_device).cpu().numpy()\n",
    "    \n",
    "    # Create a figure\n",
    "    available_digits = sorted(np.unique(labels_np[:, 0]))\n",
    "    n_digits = len(available_digits)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_digits, n_samples * 2, figsize=(n_samples * 4, 3 * n_digits))\n",
    "    if n_digits == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    fig.suptitle(\"Original Images vs. Reconstructions\", fontsize=16)\n",
    "    \n",
    "    # Plot originals and reconstructions for each digit\n",
    "    for digit_idx, digit in enumerate(available_digits):\n",
    "        # Find samples of this digit\n",
    "        indices = np.where(labels_np[:, 0] == digit)[0]\n",
    "        selected_indices = np.random.choice(indices, min(n_samples, len(indices)), replace=False)\n",
    "        \n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            # Original image\n",
    "            axes[digit_idx, i*2].imshow(data_np[idx, 0], cmap='gray')\n",
    "            axes[digit_idx, i*2].set_title(f\"Original\\nDigit {int(digit)}, {labels_np[idx, 1]*30}Â°\")\n",
    "            axes[digit_idx, i*2].axis('off')\n",
    "            \n",
    "            # Reconstruction\n",
    "            axes[digit_idx, i*2+1].imshow(reconstructions[idx, 0], cmap='gray')\n",
    "            axes[digit_idx, i*2+1].set_title(f\"Reconstruction\\nDigit {int(digit)}, {labels_np[idx, 1]*30}Â°\")\n",
    "            axes[digit_idx, i*2+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_from_latent_space(model, n_grid=15):\n",
    "    \"\"\"\n",
    "    Generate images by sampling from the latent space.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained decoder model\n",
    "        n_grid: Number of points in each dimension of the grid\n",
    "    \"\"\"\n",
    "\n",
    "    if model.latent_dim>2:\n",
    "        print('cannot generate from latent space since latent dim is >2')\n",
    "        return\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a grid of points in the latent space\n",
    "    grid_x = np.linspace(-3, 3, n_grid)\n",
    "    grid_y = np.linspace(-3, 3, n_grid)\n",
    "    \n",
    "    # Create a figure\n",
    "    figure = np.zeros((28 * n_grid, 28 * n_grid))\n",
    "    \n",
    "    # Generate images for each point in the grid\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z_sample = torch.tensor([[xi, yi]], dtype=torch.float32).to(device)\n",
    "            x_decoded = model.decode(z_sample).cpu().numpy()\n",
    "            digit = x_decoded[0, 0].reshape(28, 28)\n",
    "            figure[i * 28: (i + 1) * 28, j * 28: (j + 1) * 28] = digit\n",
    "    \n",
    "    # Plot the figure\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(figure, cmap='gray')\n",
    "    plt.title(\"Generated Images from Latent Space\", fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 7. Rotation Analysis\n",
    "def analyze_rotation_invariance(model, dataloader):\n",
    "    \"\"\"\n",
    "    Analyze rotation invariance by computing distances between\n",
    "    the same digit with different rotations in latent space.\n",
    "    \"\"\"\n",
    "    # Encode the data\n",
    "    z_mean, _, labels = encode_data(model, dataloader)\n",
    "    \n",
    "    # Project to 2D if needed\n",
    "    if z_mean.shape[1] > 2:\n",
    "        z_mean = PCA(n_components=2).fit_transform(z_mean)\n",
    "    \n",
    "    # Extract digit and rotation labels\n",
    "    digit_labels = labels[:, 0]  # Digit (1, 2, or 3)\n",
    "    rotation_labels = labels[:, 1]  # Rotation index\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'z1': z_mean[:, 0],\n",
    "        'z2': z_mean[:, 1],\n",
    "        'digit': digit_labels,\n",
    "        'rotation': rotation_labels\n",
    "    })\n",
    "    \n",
    "    # Map rotation indices to degrees\n",
    "    rotation_degrees = {idx: idx * 30 for idx in range(12)}\n",
    "    df['rotation_degrees'] = df['rotation'].map(rotation_degrees)\n",
    "    \n",
    "    # Compute the average latent position for each digit-rotation combination\n",
    "    avg_positions = df.groupby(['digit', 'rotation_degrees']).agg({\n",
    "        'z1': 'mean',\n",
    "        'z2': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Compute distances between different rotations of the same digit\n",
    "    distance_data = []\n",
    "    \n",
    "    for digit in sorted(df['digit'].unique()):\n",
    "        digit_data = avg_positions[avg_positions['digit'] == digit]\n",
    "        \n",
    "        for i, row1 in digit_data.iterrows():\n",
    "            for j, row2 in digit_data.iterrows():\n",
    "                if i != j:\n",
    "                    rot1 = row1['rotation_degrees']\n",
    "                    rot2 = row2['rotation_degrees']\n",
    "                    \n",
    "                    # Compute Euclidean distance\n",
    "                    dist = np.sqrt((row1['z1'] - row2['z1'])**2 + (row1['z2'] - row2['z2'])**2)\n",
    "                    \n",
    "                    # Compute angle difference (considering 360 degrees is the same as 0)\n",
    "                    angle_diff = min(abs(rot1 - rot2), 360 - abs(rot1 - rot2))\n",
    "                    \n",
    "                    distance_data.append({\n",
    "                        'digit': digit,\n",
    "                        'rotation1': rot1,\n",
    "                        'rotation2': rot2,\n",
    "                        'angle_diff': angle_diff,\n",
    "                        'latent_distance': dist\n",
    "                    })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    distance_df = pd.DataFrame(distance_data)\n",
    "    \n",
    "    # Plot the relationship between angle difference and latent distance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Use different colors for different digits\n",
    "    colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "    \n",
    "    # Plot for each digit\n",
    "    for i, digit in enumerate(sorted(df['digit'].unique())):\n",
    "        digit_distances = distance_df[distance_df['digit'] == digit]\n",
    "        \n",
    "        if i < len(colors):\n",
    "            color = colors[i]\n",
    "        else:\n",
    "            color = None  # Let matplotlib choose\n",
    "            \n",
    "        # Scatter plot\n",
    "        plt.scatter(\n",
    "            digit_distances['angle_diff'], \n",
    "            digit_distances['latent_distance'],\n",
    "            alpha=0.7,\n",
    "            color=color,\n",
    "            label=f\"Digit {int(digit)}\"\n",
    "        )\n",
    "        \n",
    "        # Fit a trend line\n",
    "        if len(digit_distances) > 1:\n",
    "            z = np.polyfit(digit_distances['angle_diff'], digit_distances['latent_distance'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(\n",
    "                np.sort(digit_distances['angle_diff'].unique()),\n",
    "                p(np.sort(digit_distances['angle_diff'].unique())),\n",
    "                '--', color=color\n",
    "            )\n",
    "    \n",
    "    plt.title(\"Relationship Between Rotation Angle Difference and Latent Space Distance\", fontsize=14)\n",
    "    plt.xlabel(\"Angle Difference (degrees)\", fontsize=12)\n",
    "    plt.ylabel(\"Latent Space Distance\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate correlation statistics\n",
    "    corr_results = {}\n",
    "    for digit in sorted(df['digit'].unique()):\n",
    "        digit_distances = distance_df[distance_df['digit'] == digit]\n",
    "        if len(digit_distances) > 1:\n",
    "            corr = np.corrcoef(digit_distances['angle_diff'], digit_distances['latent_distance'])[0, 1]\n",
    "            corr_results[digit] = {'correlation': corr}\n",
    "    \n",
    "    return corr_results\n",
    "\n",
    "def plot_rotation_sensitivity_heatmap(model, dataloader):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing the sensitivity to rotations in the latent space.\n",
    "    \"\"\"\n",
    "    # Encode the data\n",
    "    z_mean, _, labels = encode_data(model, dataloader)\n",
    "    \n",
    "    # Project to 2D if needed\n",
    "    if z_mean.shape[1] > 2:\n",
    "        z_mean = PCA(n_components=2).fit_transform(z_mean)\n",
    "    \n",
    "    # Extract digit and rotation labels\n",
    "    digit_labels = labels[:, 0]  # Digit (1, 2, or 3)\n",
    "    rotation_labels = labels[:, 1]  # Rotation index\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'z1': z_mean[:, 0],\n",
    "        'z2': z_mean[:, 1],\n",
    "        'digit': digit_labels,\n",
    "        'rotation': rotation_labels\n",
    "    })\n",
    "    \n",
    "    # Map rotation indices to degrees\n",
    "    rotation_degrees = {idx: idx * 30 for idx in range(12)}\n",
    "    df['rotation_degrees'] = df['rotation'].map(rotation_degrees)\n",
    "    \n",
    "    # Compute average latent positions for each digit-rotation combination\n",
    "    avg_positions = df.groupby(['digit', 'rotation_degrees']).agg({\n",
    "        'z1': 'mean',\n",
    "        'z2': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create distance matrices for each digit\n",
    "    distance_matrices = {}\n",
    "    rotation_values = sorted(avg_positions['rotation_degrees'].unique())\n",
    "    \n",
    "    unique_digits = sorted(df['digit'].unique())\n",
    "    n_digits = len(unique_digits)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_digits, figsize=(6 * n_digits, 6))\n",
    "    if n_digits == 1:\n",
    "        axes = [axes]\n",
    "    fig.suptitle(\"Rotation Sensitivity in Latent Space\", fontsize=16)\n",
    "    \n",
    "    for i, digit in enumerate(unique_digits):\n",
    "        digit_data = avg_positions[avg_positions['digit'] == digit]\n",
    "        \n",
    "        # Initialize distance matrix\n",
    "        n_rotations = len(rotation_values)\n",
    "        distance_matrix = np.zeros((n_rotations, n_rotations))\n",
    "        \n",
    "        # Fill the distance matrix\n",
    "        for i_rot, rot1 in enumerate(rotation_values):\n",
    "            for j_rot, rot2 in enumerate(rotation_values):\n",
    "                rot1_data = digit_data[digit_data['rotation_degrees'] == rot1]\n",
    "                rot2_data = digit_data[digit_data['rotation_degrees'] == rot2]\n",
    "                \n",
    "                if len(rot1_data) > 0 and len(rot2_data) > 0:\n",
    "                    pos1 = rot1_data[['z1', 'z2']].values[0]\n",
    "                    pos2 = rot2_data[['z1', 'z2']].values[0]\n",
    "                    \n",
    "                    # Compute Euclidean distance\n",
    "                    distance_matrix[i_rot, j_rot] = np.sqrt(((pos1 - pos2)**2).sum())\n",
    "        \n",
    "        distance_matrices[digit] = distance_matrix\n",
    "        \n",
    "        # Plot heatmap\n",
    "        im = axes[i].imshow(distance_matrix, cmap='viridis')\n",
    "        axes[i].set_title(f\"Digit {int(digit)}\")\n",
    "        \n",
    "        # Set ticks and labels\n",
    "        tick_positions = np.arange(len(rotation_values))\n",
    "        axes[i].set_xticks(tick_positions)\n",
    "        axes[i].set_yticks(tick_positions)\n",
    "        axes[i].set_xticklabels([f\"{int(rot)}Â°\" for rot in rotation_values])\n",
    "        axes[i].set_yticklabels([f\"{int(rot)}Â°\" for rot in rotation_values])\n",
    "        \n",
    "        # Add labels\n",
    "        axes[i].set_xlabel(\"Rotation\")\n",
    "        axes[i].set_ylabel(\"Rotation\")\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=axes[i], label=\"Latent Distance\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "    \n",
    "    return distance_matrices\n",
    "\n",
    "# 8. Latent Space Interpolation\n",
    "@torch.no_grad()\n",
    "def interpolate_in_latent_space(model, dataloader, n_steps=10):\n",
    "    \"\"\"\n",
    "    Interpolate between different digits and rotations in the latent space.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained VAE model\n",
    "        dataloader: DataLoader for the data\n",
    "        n_steps: Number of interpolation steps\n",
    "    \"\"\"\n",
    "    # Encode the data\n",
    "    z_mean, _, labels = encode_data(model, dataloader)\n",
    "    \n",
    "    # Extract digit and rotation labels\n",
    "    digit_labels = labels[:, 0]  # Digit (1, 2, or 3)\n",
    "    rotation_labels = labels[:, 1]  # Rotation index\n",
    "    \n",
    "    # Find representative samples for each digit\n",
    "    samples = {}\n",
    "    for digit in sorted(np.unique(digit_labels)):\n",
    "        for rotation in sorted(np.unique(rotation_labels)):\n",
    "            indices = np.where((digit_labels == digit) & (rotation_labels == rotation))[0]\n",
    "            if len(indices) > 0:\n",
    "                key = f\"Digit {int(digit)}, {int(rotation*30)}Â°\"\n",
    "                samples[key] = z_mean[indices[0]]\n",
    "    \n",
    "    # Choose pairs to interpolate between\n",
    "    # We'll dynamically set up pairs based on available digits\n",
    "    interpolation_pairs = []\n",
    "    \n",
    "    # Get unique digits and rotations\n",
    "    available_digits = sorted(np.unique(digit_labels))\n",
    "    \n",
    "    if len(available_digits) >= 2:\n",
    "        # Same digit, different rotation\n",
    "        digit = available_digits[0]\n",
    "        rotations = sorted(np.unique(rotation_labels))\n",
    "        if len(rotations) >= 2:\n",
    "            rot1, rot2 = rotations[0], rotations[len(rotations)//2]  # First and middle rotation\n",
    "            key1 = f\"Digit {int(digit)}, {int(rot1*30)}Â°\"\n",
    "            key2 = f\"Digit {int(digit)}, {int(rot2*30)}Â°\"\n",
    "            if key1 in samples and key2 in samples:\n",
    "                interpolation_pairs.append((key1, key2))\n",
    "        \n",
    "        # Different digit, same rotation\n",
    "        rotation = rotations[0]\n",
    "        digit1, digit2 = available_digits[0], available_digits[1]\n",
    "        key1 = f\"Digit {int(digit1)}, {int(rotation*30)}Â°\"\n",
    "        key2 = f\"Digit {int(digit2)}, {int(rotation*30)}Â°\"\n",
    "        if key1 in samples and key2 in samples:\n",
    "            interpolation_pairs.append((key1, key2))\n",
    "        \n",
    "        # Different digit, different rotation\n",
    "        if len(rotations) >= 2:\n",
    "            digit1, digit2 = available_digits[0], available_digits[1]\n",
    "            rot1, rot2 = rotations[0], rotations[len(rotations)//2]\n",
    "            key1 = f\"Digit {int(digit1)}, {int(rot1*30)}Â°\"\n",
    "            key2 = f\"Digit {int(digit2)}, {int(rot2*30)}Â°\"\n",
    "            if key1 in samples and key2 in samples:\n",
    "                interpolation_pairs.append((key1, key2))\n",
    "    \n",
    "    # Create a figure for each pair\n",
    "    for start_key, end_key in interpolation_pairs:\n",
    "        start_point = samples[start_key]\n",
    "        end_point = samples[end_key]\n",
    "        \n",
    "        # Create interpolation points\n",
    "        alphas = np.linspace(0, 1, n_steps)\n",
    "        interpolation_points = np.array([\n",
    "            start_point * (1 - alpha) + end_point * alpha\n",
    "            for alpha in alphas\n",
    "        ])\n",
    "        \n",
    "        # Convert to torch tensor\n",
    "        z_interp = torch.tensor(interpolation_points, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Generate images\n",
    "        interpolated_images = model.decode(z_interp).cpu().numpy()\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        \n",
    "        for i in range(n_steps):\n",
    "            plt.subplot(1, n_steps, i + 1)\n",
    "            plt.imshow(interpolated_images[i, 0], cmap='gray')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            if i == 0:\n",
    "                plt.title(start_key, fontsize=8)\n",
    "            elif i == n_steps - 1:\n",
    "                plt.title(end_key, fontsize=8)\n",
    "            else:\n",
    "                plt.title(f\"{(i / (n_steps-1) * 100):.0f}%\", fontsize=8)\n",
    "        \n",
    "        plt.suptitle(f\"Latent Space Interpolation: {start_key} â {end_key}\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.85)\n",
    "        plt.show()\n",
    "\n",
    "# 9. Model Saving and Loading\n",
    "def save_model(model, path='models/vae_model.pth'):\n",
    "    \"\"\"Save the model to a file\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model_type, latent_dim, path='models/vae_model.pth'):\n",
    "    \"\"\"Load a model from a file\"\"\"\n",
    "    model = create_vae_model(model_type, latent_dim)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model\n",
    "\n",
    "class LatentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Oracle classifier that operates on the latent space of a VAE\n",
    "    to classify the digit.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, num_classes, hidden_dim=128):\n",
    "        super(LatentClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"Classify a latent vector z to predict the digit\"\"\"\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "def prepare_classification_data(original_data, batch_size=128, test_ratio=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Prepare data for classification directly from the original data.\n",
    "    \n",
    "    Args:\n",
    "        original_data: Dictionary containing the original data from load_data\n",
    "        batch_size: Batch size for the DataLoader\n",
    "        test_ratio: Ratio of data to use for testing\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        train_loader, test_loader: DataLoaders for training and testing\n",
    "    \"\"\"\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Process each digit in the data\n",
    "    for digit_idx, digit_key in enumerate(sorted(original_data.keys())):\n",
    "        digit_data = original_data[digit_key]\n",
    "        digit_number = int(digit_key.split('_')[1])  # Extract digit number from key\n",
    "        \n",
    "        # Get dimensions\n",
    "        n_samples, n_rotations, _, h, w = digit_data.shape\n",
    "        \n",
    "        # For each sample of this digit\n",
    "        for sample_idx in range(n_samples):\n",
    "            # For each rotation of this sample\n",
    "            for rot_idx in range(n_rotations):\n",
    "                img = digit_data[sample_idx, rot_idx, 0]\n",
    "                \n",
    "                # Store image and digit label\n",
    "                all_images.append(img)\n",
    "                all_labels.append(digit_number-1)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_images = np.array(all_images)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Reshape for PyTorch CNN (batch, channels, height, width)\n",
    "    all_images = all_images.reshape(-1, 1, h, w)\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    all_images = all_images.astype('float32') / np.max(all_images)\n",
    "    \n",
    "    # Split into train/test\n",
    "    indices = np.arange(len(all_images))\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    test_size = int(len(indices) * test_ratio)\n",
    "    train_indices = indices[test_size:]\n",
    "    test_indices = indices[:test_size]\n",
    "    \n",
    "    # Create tensors\n",
    "    X_train = torch.tensor(all_images[train_indices], dtype=torch.float32)\n",
    "    y_train = torch.tensor(all_labels[train_indices], dtype=torch.long)\n",
    "    \n",
    "    X_test = torch.tensor(all_images[test_indices], dtype=torch.float32)\n",
    "    y_test = torch.tensor(all_labels[test_indices], dtype=torch.long)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_dataset(vae_model, data_loader):\n",
    "    \"\"\"\n",
    "    Encode a dataset to mean and log variance in latent space.\n",
    "    \n",
    "    Args:\n",
    "        vae_model: VAE model for encoding\n",
    "        data_loader: DataLoader with data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (mu, log_var, labels)\n",
    "    \"\"\"\n",
    "    vae_model.eval()\n",
    "    \n",
    "    all_mu = []\n",
    "    all_log_var = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for data, labels in tqdm(data_loader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Encode to latent space parameters\n",
    "        mu, log_var = vae_model.encode(data)\n",
    "        \n",
    "        all_mu.append(mu.cpu())\n",
    "        all_log_var.append(log_var.cpu())\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    # Concatenate results\n",
    "    return torch.cat(all_mu, dim=0), torch.cat(all_log_var, dim=0), torch.cat(all_labels, dim=0)\n",
    "\n",
    "\n",
    "def train_latent_classifier(vae_model, classifier_model, mu_train, log_var_train, y_train, \n",
    "                          mu_val=None, log_var_val=None, y_val=None, \n",
    "                          epochs=20, lr=1e-3, batch_size=128, patience=5):\n",
    "    \"\"\"\n",
    "    Train the latent classifier, sampling from the latent distribution for each batch.\n",
    "    \n",
    "    Args:\n",
    "        vae_model: VAE model for reparameterization\n",
    "        classifier_model: Classifier model to train\n",
    "        mu_train: Mean vectors for training data\n",
    "        log_var_train: Log variance vectors for training data\n",
    "        y_train: Training labels\n",
    "        mu_val: Optional validation mean vectors\n",
    "        log_var_val: Optional validation log variance vectors\n",
    "        y_val: Optional validation labels\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        batch_size: Batch size\n",
    "        patience: Patience for early stopping\n",
    "    \"\"\"\n",
    "    # Create dataloaders\n",
    "    train_dataset = TensorDataset(mu_train, log_var_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    val_loader = None\n",
    "    if mu_val is not None and log_var_val is not None and y_val is not None:\n",
    "        val_dataset = TensorDataset(mu_val, log_var_val, y_val)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = optim.Adam(classifier_model.parameters(), lr=lr)\n",
    "    classifier_model.train()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        \"loss\": [],\n",
    "        \"accuracy\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_accuracy\": []\n",
    "    }\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for mu_batch, log_var_batch, y_batch in train_loader:\n",
    "            mu_batch = mu_batch.to(device)\n",
    "            log_var_batch = log_var_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Sample from latent distribution for this batch\n",
    "            z_batch = vae_model.reparameterize(mu_batch, log_var_batch)\n",
    "            \n",
    "            # Forward pass through classifier\n",
    "            outputs = []\n",
    "            for _ in range(11):\n",
    "                z_batch = transform_model(z_batch)\n",
    "                outputs.append(z_batch)\n",
    "                averaged_output = torch.stack(outputs).mean(dim=0)\n",
    "            outputs = classifier_model(averaged_output)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * mu_batch.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        history[\"loss\"].append(epoch_loss)\n",
    "        history[\"accuracy\"].append(epoch_acc)\n",
    "        \n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "            val_loss, val_acc = evaluate_classifier(vae_model, classifier_model, val_loader, criterion)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            history[\"val_accuracy\"].append(val_acc)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = classifier_model.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    classifier_model.load_state_dict(best_model_state)\n",
    "                    break\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Acc: {epoch_acc:.4f} - \" \n",
    "                  f\"Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Acc: {epoch_acc:.4f}\")\n",
    "    \n",
    "    # Restore best model if we did validation\n",
    "    if val_loader is not None and best_model_state is not None:\n",
    "        classifier_model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_classifier(vae_model, classifier_model, data_loader, criterion=None):\n",
    "    \"\"\"\n",
    "    Evaluate the classifier on a dataset by sampling from latent distributions.\n",
    "    \n",
    "    Args:\n",
    "        vae_model: VAE model for reparameterization\n",
    "        classifier_model: Classifier model to evaluate\n",
    "        data_loader: DataLoader with mu, log_var, and labels\n",
    "        criterion: Optional loss function\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (average loss, accuracy)\n",
    "    \"\"\"\n",
    "    classifier_model.eval()\n",
    "    vae_model.eval()\n",
    "    \n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for mu_batch, log_var_batch, y_batch in data_loader:\n",
    "        mu_batch = mu_batch.to(device)\n",
    "        log_var_batch = log_var_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # Sample from latent distribution\n",
    "        z_batch = vae_model.reparameterize(mu_batch, log_var_batch)\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = []\n",
    "        for _ in range(11):\n",
    "            z_batch = transform_model(z_batch)\n",
    "            outputs.append(z_batch)\n",
    "            averaged_output = torch.stack(outputs).mean(dim=0)\n",
    "        outputs = classifier_model(averaged_output)\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        running_loss += loss.item() * mu_batch.size(0)\n",
    "        \n",
    "        # Accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "\n",
    "# Functions to save/load models\n",
    "def save_classifier(classifier_model, path='models/classifier_model.pth'):\n",
    "    \"\"\"Save the classifier model to a file\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(classifier_model.state_dict(), path)\n",
    "    print(f\"Classifier model saved to {path}\")\n",
    "\n",
    "def load_classifier(latent_dim, num_classes, path='models/classifier_model.pth'):\n",
    "    \"\"\"Load a classifier model from a file\"\"\"\n",
    "    classifier_model = LatentClassifier(latent_dim, num_classes).to(device)\n",
    "    classifier_model.load_state_dict(torch.load(path, map_location=device))\n",
    "    classifier_model.eval()\n",
    "    print(f\"Classifier model loaded from {path}\")\n",
    "    return classifier_model\n",
    "\n",
    "\n",
    "# Main function for classification analysis with pre-trained VAE\n",
    "def run_classifier_with_pretrained_vae(\n",
    "    file_path,\n",
    "    digits=['digit_1','digit_2','digit_3','digit_4','digit_5','digit_6','digit_7','digit_8','digit_9','digit_10'],\n",
    "    vae_model_path='models/vae_model.pth',\n",
    "    model_type=\"conv\",\n",
    "    latent_dim=2,\n",
    "    batch_size=128,\n",
    "    classifier_epochs=20,\n",
    "    classifier_lr=1e-3,\n",
    "    load_existing_classifier=False,\n",
    "    classifier_model_path='models/classifier_model.pth'\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the classifier analysis with a pre-trained VAE.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the HDF5 file with rotated digits\n",
    "        digits: List of digit keys to use\n",
    "        vae_model_path: Path to the pre-trained VAE model\n",
    "        model_type: Type of VAE model (\"conv\" or \"deep\")\n",
    "        latent_dim: Dimensionality of the latent space\n",
    "        batch_size: Batch size for training\n",
    "        classifier_epochs: Number of epochs for training the classifier\n",
    "        classifier_lr: Learning rate for the classifier\n",
    "        load_existing_classifier: Whether to load an existing classifier model\n",
    "        classifier_model_path: Path to load/save the classifier model\n",
    "    \"\"\"\n",
    "    # Set the seed for reproducibility\n",
    "    set_seed(42)\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data(file_path, digits)\n",
    "    \n",
    "    # 2. Prepare data directly from original dataset\n",
    "    print(\"Preparing classification data...\")\n",
    "    train_loader, test_loader = prepare_classification_data(data, batch_size)\n",
    "    \n",
    "    # 3. Load pre-trained VAE model\n",
    "    print(f\"Loading pre-trained {model_type} VAE model...\")\n",
    "    vae_model = load_model(model_type, latent_dim, vae_model_path)\n",
    "    \n",
    "    # 4. Encode the datasets to mu and log_var\n",
    "    print(\"Encoding datasets to latent parameters (mu, log_var)...\")\n",
    "    mu_train, log_var_train, y_train = encode_dataset(vae_model, train_loader)\n",
    "    mu_test, log_var_test, y_test = encode_dataset(vae_model, test_loader)\n",
    "    \n",
    "    # Count number of unique digits\n",
    "    num_classes = len(torch.unique(y_train))\n",
    "    print(f\"Training classifier for {num_classes} digit classes\")\n",
    "    \n",
    "    # 5. Create or load the classifier\n",
    "    if load_existing_classifier and os.path.exists(classifier_model_path):\n",
    "        print(\"Loading existing classifier model...\")\n",
    "        classifier_model = load_classifier(latent_dim, num_classes, classifier_model_path)\n",
    "    else:\n",
    "        print(\"Creating and training new classifier model...\")\n",
    "        classifier_model = LatentClassifier(latent_dim, num_classes,hidden_dim=1024).to(device)\n",
    "        \n",
    "        # 6. Train the classifier, sampling from the latent distribution each time\n",
    "        history = train_latent_classifier(\n",
    "            vae_model,\n",
    "            classifier_model,\n",
    "            mu_train, log_var_train, y_train,\n",
    "            mu_test, log_var_test, y_test,\n",
    "            epochs=classifier_epochs,\n",
    "            lr=classifier_lr,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # 7. Save classifier model\n",
    "        save_classifier(classifier_model, classifier_model_path)\n",
    "    \n",
    "    # 8. Evaluate on test set\n",
    "    test_dataset = TensorDataset(mu_test, log_var_test, y_test)\n",
    "    test_loader_encoded = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loss, test_accuracy = evaluate_classifier(vae_model, classifier_model, test_loader_encoded)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return vae_model, classifier_model, (mu_test, log_var_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e42744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:09:44.729837Z",
     "iopub.status.busy": "2025-04-01T12:09:44.729405Z",
     "iopub.status.idle": "2025-04-01T12:22:10.996655Z",
     "shell.execute_reply": "2025-04-01T12:22:10.995605Z"
    },
    "papermill": {
     "duration": 746.272152,
     "end_time": "2025-04-01T12:22:10.998045",
     "exception": false,
     "start_time": "2025-04-01T12:09:44.725893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Loading data...\n",
      "Available keys: ['digit_0', 'digit_1', 'digit_2', 'digit_3', 'digit_4', 'digit_5', 'digit_6', 'digit_7', 'digit_8', 'digit_9']\n",
      "Preparing classification data...\n",
      "Loading pre-trained deep VAE model...\n",
      "Model loaded from /kaggle/input/deep-16-10-digit/models/vae_model.pth\n",
      "Encoding datasets to latent parameters (mu, log_var)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-ccc850f39093>:1042: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path, map_location=device))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12252b67ff3c49669a009d1bf7d3c6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1d25afc7f34daf83374b69c8928842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1014 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier for 9 digit classes\n",
      "Creating and training new classifier model...\n",
      "Epoch 1/20 - Loss: 0.7556 - Acc: 0.7414 - Val Loss: 0.6288 - Val Acc: 0.7883\n",
      "Epoch 2/20 - Loss: 0.5694 - Acc: 0.8105 - Val Loss: 0.5317 - Val Acc: 0.8246\n",
      "Epoch 3/20 - Loss: 0.4976 - Acc: 0.8351 - Val Loss: 0.4905 - Val Acc: 0.8397\n",
      "Epoch 4/20 - Loss: 0.4607 - Acc: 0.8478 - Val Loss: 0.4694 - Val Acc: 0.8463\n",
      "Epoch 5/20 - Loss: 0.4394 - Acc: 0.8551 - Val Loss: 0.4573 - Val Acc: 0.8510\n",
      "Epoch 6/20 - Loss: 0.4214 - Acc: 0.8611 - Val Loss: 0.4443 - Val Acc: 0.8550\n",
      "Epoch 7/20 - Loss: 0.4108 - Acc: 0.8647 - Val Loss: 0.4346 - Val Acc: 0.8587\n",
      "Epoch 8/20 - Loss: 0.3993 - Acc: 0.8679 - Val Loss: 0.4279 - Val Acc: 0.8607\n",
      "Epoch 9/20 - Loss: 0.3910 - Acc: 0.8708 - Val Loss: 0.4214 - Val Acc: 0.8627\n",
      "Epoch 10/20 - Loss: 0.3818 - Acc: 0.8737 - Val Loss: 0.4187 - Val Acc: 0.8640\n",
      "Epoch 11/20 - Loss: 0.3770 - Acc: 0.8752 - Val Loss: 0.4136 - Val Acc: 0.8657\n",
      "Epoch 12/20 - Loss: 0.3687 - Acc: 0.8780 - Val Loss: 0.4198 - Val Acc: 0.8630\n",
      "Epoch 13/20 - Loss: 0.3657 - Acc: 0.8788 - Val Loss: 0.4113 - Val Acc: 0.8661\n",
      "Epoch 14/20 - Loss: 0.3609 - Acc: 0.8805 - Val Loss: 0.4058 - Val Acc: 0.8697\n",
      "Epoch 15/20 - Loss: 0.3568 - Acc: 0.8818 - Val Loss: 0.4115 - Val Acc: 0.8675\n",
      "Epoch 16/20 - Loss: 0.3519 - Acc: 0.8833 - Val Loss: 0.4059 - Val Acc: 0.8695\n",
      "Epoch 17/20 - Loss: 0.3476 - Acc: 0.8843 - Val Loss: 0.4073 - Val Acc: 0.8695\n",
      "Epoch 18/20 - Loss: 0.3449 - Acc: 0.8855 - Val Loss: 0.4013 - Val Acc: 0.8721\n",
      "Epoch 19/20 - Loss: 0.3410 - Acc: 0.8869 - Val Loss: 0.4042 - Val Acc: 0.8701\n",
      "Epoch 20/20 - Loss: 0.3372 - Acc: 0.8879 - Val Loss: 0.4053 - Val Acc: 0.8703\n",
      "Classifier model saved to models/classifier_model.pth\n",
      "Test Loss: 0.4027, Test Accuracy: 0.8715\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual file path\n",
    "    file_path = \"/kaggle/input/e2e-specific-task-1-dataset/rotated_digits.hdf5\"\n",
    "    \n",
    "    # Path to pre-trained VAE model\n",
    "    vae_model_path = '/kaggle/input/deep-16-10-digit/models/vae_model.pth'\n",
    "    \n",
    "    # Run the classifier with pre-trained VAE\n",
    "    vae_model, classifier_model, test_data = run_classifier_with_pretrained_vae(\n",
    "        file_path,\n",
    "        # digits=['digit_1', 'digit_2'],\n",
    "        vae_model_path=vae_model_path,\n",
    "        model_type=\"deep\",\n",
    "        latent_dim=16,\n",
    "        batch_size=128,\n",
    "        classifier_epochs=20,\n",
    "        classifier_lr=1e-3,\n",
    "        load_existing_classifier=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1083904",
   "metadata": {
    "papermill": {
     "duration": 0.004241,
     "end_time": "2025-04-01T12:22:11.007010",
     "exception": false,
     "start_time": "2025-04-01T12:22:11.002769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c6c34",
   "metadata": {
    "papermill": {
     "duration": 0.004134,
     "end_time": "2025-04-01T12:22:11.016062",
     "exception": false,
     "start_time": "2025-04-01T12:22:11.011928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb7d371",
   "metadata": {
    "papermill": {
     "duration": 0.004105,
     "end_time": "2025-04-01T12:22:11.024340",
     "exception": false,
     "start_time": "2025-04-01T12:22:11.020235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7003455,
     "sourceId": 11215337,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6814795,
     "sourceId": 11236618,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7019878,
     "sourceId": 11236796,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7019889,
     "sourceId": 11236810,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7019898,
     "sourceId": 11236819,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7021882,
     "sourceId": 11239453,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7022288,
     "sourceId": 11239986,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 756.806098,
   "end_time": "2025-04-01T12:22:12.650766",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-01T12:09:35.844668",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00fff6e2c7854f9497f5718b89af0026": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_87a3ffe845564bd59ac3ffca3c1d258b",
       "placeholder": "â",
       "style": "IPY_MODEL_8b9cfb504b9a428dab0907374f4264fa",
       "tabbable": null,
       "tooltip": null,
       "value": "â1014/1014â[00:02&lt;00:00,â415.61it/s]"
      }
     },
     "0df039926ccb4363893e7d2642536725": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_aaac2cf4a6b24500918b0f24a66498de",
       "max": 1014.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d7b709028d9244d5a3c1b145305c074e",
       "tabbable": null,
       "tooltip": null,
       "value": 1014.0
      }
     },
     "12252b67ff3c49669a009d1bf7d3c6a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ccd7862b87874b8686d90c661c9132d2",
        "IPY_MODEL_ef87661211ba40db871a10916780e637",
        "IPY_MODEL_4870c9aecaf544858f7fbf534fdc275e"
       ],
       "layout": "IPY_MODEL_7dc914a3a42b46868b93271be73280e3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "13c311510c9f4b14b37ebd6b730c9100": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "16951565a47043c8ad25cd98e98d4d44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "35025da8448a4ce7a2af08aa82790d40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3b1dfacf5fd2474b84c8f5439f29b586": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4870c9aecaf544858f7fbf534fdc275e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_13c311510c9f4b14b37ebd6b730c9100",
       "placeholder": "â",
       "style": "IPY_MODEL_16951565a47043c8ad25cd98e98d4d44",
       "tabbable": null,
       "tooltip": null,
       "value": "â4056/4056â[00:10&lt;00:00,â406.67it/s]"
      }
     },
     "4fd8f3977c624eb89b900ecdacb9dc8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6e24b74582b84cf9997f1d821e9b138f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7dc914a3a42b46868b93271be73280e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "87a3ffe845564bd59ac3ffca3c1d258b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8b9cfb504b9a428dab0907374f4264fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "aaac2cf4a6b24500918b0f24a66498de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9f3ba1408fd4f278e04cc893986f2ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c81d5e02a4ae413ab339282af6d824dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cc1d25afc7f34daf83374b69c8928842": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fbed0f458eab407f869b22f2e052cb09",
        "IPY_MODEL_0df039926ccb4363893e7d2642536725",
        "IPY_MODEL_00fff6e2c7854f9497f5718b89af0026"
       ],
       "layout": "IPY_MODEL_b9f3ba1408fd4f278e04cc893986f2ea",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ccd7862b87874b8686d90c661c9132d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6e24b74582b84cf9997f1d821e9b138f",
       "placeholder": "â",
       "style": "IPY_MODEL_3b1dfacf5fd2474b84c8f5439f29b586",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "d7b709028d9244d5a3c1b145305c074e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "eb8c5d8f84c44b278d3e97a14cff8d19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ef87661211ba40db871a10916780e637": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4fd8f3977c624eb89b900ecdacb9dc8c",
       "max": 4056.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c81d5e02a4ae413ab339282af6d824dd",
       "tabbable": null,
       "tooltip": null,
       "value": 4056.0
      }
     },
     "fbed0f458eab407f869b22f2e052cb09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_35025da8448a4ce7a2af08aa82790d40",
       "placeholder": "â",
       "style": "IPY_MODEL_eb8c5d8f84c44b278d3e97a14cff8d19",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
